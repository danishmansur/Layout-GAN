{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"complete loading pre_dat.npy\")? (<ipython-input-1-8a965e4c4a24>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8a965e4c4a24>\"\u001b[1;36m, line \u001b[1;32m50\u001b[0m\n\u001b[1;33m    print \"complete loading pre_dat.npy\"\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"complete loading pre_dat.npy\")?\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import random\n",
    "\n",
    "from ops import *\n",
    "from utils import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "class LAYOUTGAN(object):\n",
    "  def __init__(self, sess, batch_size=64, sample_num=64, dataset_name='default', checkpoint_dir=None, sample_dir=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      sess: TensorFlow session\n",
    "      batch_size: The size of batch.\n",
    "    \"\"\"\n",
    "    self.sess = sess\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.sample_num = sample_num\n",
    "    self.dataset_name = dataset_name\n",
    "    self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    self.d_bn0 = batch_norm(name='d_bn0')\n",
    "    self.d_bn1 = batch_norm(name='d_bn1')\n",
    "    self.d_bn2 = batch_norm(name='d_bn2')\n",
    "\n",
    "    self.g_bn0_0 = batch_norm(name='g_bn0_0')\n",
    "    self.g_bn0_1 = batch_norm(name='g_bn0_1')\n",
    "    self.g_bn0_2 = batch_norm(name='g_bn0_2')\n",
    "    self.g_bn0_3 = batch_norm(name='g_bn0_3')\n",
    "    self.g_bn1_0 = batch_norm(name='g_bn1_0')\n",
    "    self.g_bn1_1 = batch_norm(name='g_bn1_1')\n",
    "    self.g_bn1_2 = batch_norm(name='g_bn1_2')\n",
    "    self.g_bn1_3 = batch_norm(name='g_bn1_3')\n",
    "\n",
    "    self.g_bn_x0 = batch_norm(name='g_bn_x0')\n",
    "    self.g_bn_x1 = batch_norm(name='g_bn_x1')\n",
    "    self.g_bn_x2 = batch_norm(name='g_bn_x2')\n",
    "    self.g_bn_x3 = batch_norm(name='g_bn_x3')\n",
    "\n",
    "\n",
    "    self.data_pre = np.load('./data/pre_data_cls.npy')\n",
    "    print \"complete loading pre_dat.npy\"\n",
    "    print len(self.data_pre) \n",
    "\n",
    "    self.build_model()\n",
    "\n",
    "\n",
    "  def build_model(self):\n",
    "\n",
    "    self.inputs = tf.placeholder(tf.float32, [self.batch_size, 128, 2], name='real_images')\n",
    "    self.z = tf.placeholder(tf.float32, [64, 128, 2], name='z')\n",
    "\n",
    "    self.G = self.generator(self.z)\n",
    "    self.D, self.D_logits = self.discriminator(self.inputs, reuse=False)\n",
    "    self.sampler = self.sampler(self.z)\n",
    "    self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n",
    "    \n",
    "    def sigmoid_cross_entropy_with_logits(x, y):\n",
    "      try:\n",
    "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n",
    "      except:\n",
    "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n",
    "\n",
    "    self.d_loss_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n",
    "    self.d_loss_fake = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n",
    "    self.d_loss = self.d_loss_real + self.d_loss_fake\n",
    "    self.g_loss = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n",
    "\n",
    "    self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
    "    self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
    "\n",
    "    self.saver = tf.train.Saver()\n",
    "\n",
    "  def train(self, config):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    epoch_step = len(self.data_pre) // config.batch_size    \n",
    "    lr = tf.train.exponential_decay(0.00001, global_step, 20*epoch_step, 0.1, staircase=True)\n",
    "\n",
    "    d_optim = tf.train.AdamOptimizer(lr, beta1=0.9).minimize(self.d_loss, var_list=self.d_vars, global_step=global_step)\n",
    "    g_optim = tf.train.AdamOptimizer(lr, beta1=0.9).minimize(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "    try:\n",
    "      tf.global_variables_initializer().run()\n",
    "    except:\n",
    "      tf.initialize_all_variables().run()\n",
    "    \n",
    "    sample = self.data_pre[0:self.sample_num]\n",
    "    sample_inputs = np.array(sample).astype(np.float32)\n",
    "    sample_inputs = sample_inputs * 28.0 / 27.0 \n",
    "\n",
    "    sample_z = np.random.normal(0.5, 0.15, (64, 128, 2))\n",
    "  \n",
    "    counter = 1\n",
    "    start_time = time.time()\n",
    "    could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "    if could_load:\n",
    "      counter = checkpoint_counter\n",
    "      print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "      print(\" [!] Load failed...\")\n",
    "\n",
    "    for epoch in xrange(config.epoch):\n",
    "      np.random.shuffle(self.data_pre)\n",
    "      batch_idxs = len(self.data_pre) // config.batch_size\n",
    "\n",
    "      for idx in xrange(0, batch_idxs):\n",
    "        batch = self.data_pre[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "        batch_images = np.array(batch).astype(np.float32)\n",
    "        batch_images = batch_images * 28.0 / 27.0 \n",
    "\n",
    "        batch_z = np.random.normal(0.5, 0.15, (64, 128, 2))\n",
    "\n",
    "        # Update D network\n",
    "        _ = self.sess.run([d_optim], feed_dict={ self.inputs: batch_images, self.z: batch_z})\n",
    "\n",
    "        # Update G network\n",
    "        _ = self.sess.run([g_optim], feed_dict={ self.inputs: batch_images, self.z: batch_z})\n",
    "        _ = self.sess.run([g_optim], feed_dict={ self.inputs: batch_images, self.z: batch_z})\n",
    "\n",
    "        errD_fake = self.d_loss_fake.eval({ self.z: batch_z})\n",
    "        errD_real = self.d_loss_real.eval({ self.inputs: batch_images})\n",
    "        errG = self.g_loss.eval({self.inputs: batch_images, self.z: batch_z})\n",
    "\n",
    "        counter += 1\n",
    "        if np.mod(counter, 10) == 0: \n",
    "          print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, lr:%.8f, d_loss: %.4f, g_loss: %.4f\" \\\n",
    "            % (epoch, idx, batch_idxs, time.time()-start_time, lr.eval(), errD_fake+errD_real, errG))\n",
    "\n",
    "        if np.mod(counter, 200) == 1:\n",
    "          samples, d_loss, g_loss = self.sess.run([self.sampler, self.d_loss, self.g_loss],\n",
    "            feed_dict={self.z: sample_z, self.inputs: sample_inputs})\n",
    "\n",
    "          samples = np.reshape(samples, (64, 128, 2))\n",
    "          samples = 27.0 * samples\n",
    "          img_all = np.zeros((64, 28, 28, 3), dtype=np.uint8)\n",
    "\n",
    "          for img_ind in range(64):\n",
    "            pointset = np.rint(samples[img_ind,:,:]).astype(np.int)\n",
    "            pointset = pointset[~(pointset==0).all(1)]\n",
    "           \n",
    "            img = np.zeros((28,28), dtype=np.float32)\n",
    "            img[pointset[:,0], pointset[:,1]] = 255\n",
    "            img = Image.fromarray(img.astype('uint8'), 'L')\n",
    "\n",
    "            img_all[img_ind, :, :, :] = np.array(img.convert('RGB'))\n",
    "\n",
    "          img_all = np.squeeze(merge(img_all, image_manifold_size(samples.shape[0])))\n",
    "\n",
    "          scipy.misc.imsave('./{}/train_{:02d}_{:04d}.jpg'.format(config.sample_dir, epoch, idx), img_all)\n",
    "          print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) \n",
    "\n",
    "        if np.mod(counter, 2000) == 0:\n",
    "          self.save(config.checkpoint_dir, counter)\n",
    "\n",
    "\n",
    "  def discriminator(self, image, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "      if reuse:\n",
    "        scope.reuse_variables()\n",
    "\n",
    "      layout = layout_point(image, 28, 28, name='layout')\n",
    "      # For bbox layout generation\n",
    "      # layout = layout_bbox(image, 60, 40, name='layout')\n",
    "\n",
    "      net = lrelu(self.d_bn0(conv2d(layout, 32, k_h=5, k_w=5, d_h=2, d_w=2, padding='VALID', name='conv1')))\n",
    "      net = lrelu(self.d_bn1(conv2d(net, 64, k_h=5, k_w=5, d_h=2, d_w=2, padding='VALID', name='conv2')))\n",
    "      net = tf.reshape(net, [self.batch_size, -1])      \n",
    "      net = lrelu(self.d_bn2(linear(net, 512, scope='fc2')))\n",
    "      net = linear(net, 1, 'fc3')\n",
    "\n",
    "    return tf.nn.sigmoid(net), net\n",
    "\n",
    "\n",
    "  def generator(self, z):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "      gnet = tf.reshape(z, [64, 128, 1, 2])\n",
    "      h0_0 = self.g_bn0_0(conv2d(gnet, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_0'))\n",
    "      h0_1 = tf.nn.relu(self.g_bn0_1(conv2d(gnet, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_1')))\n",
    "      h0_2 = tf.nn.relu(self.g_bn0_2(conv2d(h0_1, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_2')))\n",
    "      h0_3 = self.g_bn0_3(conv2d(h0_2, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_3'))\n",
    "      gnet = tf.nn.relu(tf.add(h0_0, h0_3))\n",
    "\n",
    "      # For bbox layout generation\n",
    "      # gnet = tf.reshape(z, [64, 9, 6, 4])\n",
    "      # h0_0 = self.g_bn0_0(conv2d(gnet, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_0'))\n",
    "      # h0_1 = tf.nn.relu(self.g_bn0_1(conv2d(gnet, 64, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_1')))\n",
    "      # h0_2 = tf.nn.relu(self.g_bn0_2(conv2d(h0_1, 64, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_2')))\n",
    "      # h0_3 = self.g_bn0_3(conv2d(h0_2, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_3'))\n",
    "      # gnet = tf.nn.relu(tf.add(h0_0, h0_3))\n",
    "      # gnet = tf.reshape(gnet, [64, 9, 1, 6*256])\n",
    "\n",
    "      gnet = tf.reshape(gnet, [64, 128, 1, 1024])\n",
    "      gnet = tf.nn.relu(self.g_bn_x1( tf.add(gnet, self.g_bn_x0(relation_nonLocal(gnet, name='g_non0')))))\n",
    "      gnet = tf.nn.relu(self.g_bn_x3( tf.add(gnet, self.g_bn_x2(relation_nonLocal(gnet, name='g_non2')))))\n",
    "\n",
    "      h1_0 = self.g_bn1_0(conv2d(gnet, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_0'))\n",
    "      h1_1 = tf.nn.relu(self.g_bn1_1(conv2d(h1_0, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_1')))\n",
    "      h1_2 = tf.nn.relu(self.g_bn1_2(conv2d(h1_1, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_2')))\n",
    "      h1_3 = self.g_bn1_3(conv2d(h1_2, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_3'))\n",
    "      gnet = tf.nn.relu(tf.add(h1_0, h1_3))\n",
    "\n",
    "      # For bbox layout generation\n",
    "      # May add more self-attention refinement steps\n",
    "\n",
    "      bbox_pred = conv2d(gnet, 2, k_h=1, k_w=1, d_h=1, d_w=1, name='bbox_pred')\n",
    "      bbox_pred = tf.sigmoid(tf.reshape(bbox_pred, [-1, 128, 2]))\n",
    "      final_pred = bbox_pred\n",
    "\n",
    "      # For bbox layout generation \n",
    "      # cls_score = conv2d(gnet, 6, k_h=1, k_w=1, d_h=1, d_w=1, name='cls_score')\n",
    "      # cls_prob  = tf.sigmoid(tf.reshape(cls_score, [-1, 9, 6]))\n",
    "      # final_pred = tf.concat([bbox_pred, cls_prob], axis=-1)\n",
    "\n",
    "      return final_pred \n",
    "\n",
    "\n",
    "  def sampler(self, z):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "      scope.reuse_variables()\n",
    "      gnet = tf.reshape(z, [64, 128, 1, 2])\n",
    "      h0_0 = self.g_bn0_0(conv2d(gnet, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_0'), train=False)\n",
    "      h0_1 = tf.nn.relu(self.g_bn0_1(conv2d(gnet, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_1'), train=False))\n",
    "      h0_2 = tf.nn.relu(self.g_bn0_2(conv2d(h0_1, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_2'), train=False))\n",
    "      h0_3 = self.g_bn0_3(conv2d(h0_2, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h0_3'), train=False)\n",
    "      gnet = tf.nn.relu(tf.add(h0_0, h0_3))\n",
    "\n",
    "      gnet = tf.reshape(gnet, [64, 128, 1, 1024])\n",
    "      gnet = tf.nn.relu(self.g_bn_x1( tf.add(gnet, self.g_bn_x0(relation_nonLocal(gnet, name='g_non0'), train=False)), train=False))\n",
    "      gnet = tf.nn.relu(self.g_bn_x3( tf.add(gnet, self.g_bn_x2(relation_nonLocal(gnet, name='g_non2'), train=False)), train=False))\n",
    "\n",
    "      h1_0 = self.g_bn1_0(conv2d(gnet, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_0'), train=False)\n",
    "      h1_1 = tf.nn.relu(self.g_bn1_1(conv2d(h1_0, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_1'), train=False))\n",
    "      h1_2 = tf.nn.relu(self.g_bn1_2(conv2d(h1_1, 256, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_2'), train=False))\n",
    "      h1_3 = self.g_bn1_3(conv2d(h1_2, 1024, k_h=1, k_w=1, d_h=1, d_w=1, name='g_h1_3'), train=False)\n",
    "      gnet = tf.nn.relu(tf.add(h1_0, h1_3))\n",
    "\n",
    "      bbox_pred = conv2d(gnet, 2, k_h=1, k_w=1, d_h=1, d_w=1, name='bbox_pred')\n",
    "      bbox_pred = tf.sigmoid(tf.reshape(bbox_pred, [-1, 128, 2]))\n",
    "      final_pred = bbox_pred\n",
    "\n",
    "      return final_pred \n",
    "\n",
    "\n",
    "  @property\n",
    "  def model_dir(self):\n",
    "    return \"{}_{}\".format(self.dataset_name, self.batch_size)\n",
    "      \n",
    "  def save(self, checkpoint_dir, step):\n",
    "    model_name = \"LAYOUTGAN.model\"\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "      os.makedirs(checkpoint_dir)\n",
    "\n",
    "    self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step)\n",
    "\n",
    "  def load(self, checkpoint_dir):\n",
    "    import re\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "      counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "      print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "      return True, counter\n",
    "    else:\n",
    "      print(\" [*] Failed to find a checkpoint\")\n",
    "      return False, 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
